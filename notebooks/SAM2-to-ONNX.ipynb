{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52dacc2d-6566-4c49-beaa-ea04407e4fa0",
   "metadata": {},
   "source": [
    "# Download official SAM2 weights and modeling code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86df18d1-89a4-4fba-bb50-a30ecd2d5656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-11-15 09:45:01--  https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_tiny.pt\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.32.110.129, 13.32.110.12, 13.32.110.46, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.32.110.129|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 155906050 (149M) [application/vnd.snesdev-page-table]\n",
      "Saving to: ‘sam2_hiera_tiny.pt’\n",
      "\n",
      "sam2_hiera_tiny.pt  100%[===================>] 148.68M  47.5MB/s    in 3.1s    \n",
      "\n",
      "2024-11-15 09:45:04 (47.5 MB/s) - ‘sam2_hiera_tiny.pt’ saved [155906050/155906050]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_tiny.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b442560-5250-4280-8213-6bb58544459a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'segment-anything-2'...\n",
      "remote: Enumerating objects: 974, done.\u001b[K\n",
      "remote: Counting objects: 100% (38/38), done.\u001b[K\n",
      "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
      "remote: Total 974 (delta 17), reused 18 (delta 6), pack-reused 936 (from 1)\u001b[K\n",
      "Receiving objects: 100% (974/974), 128.94 MiB | 13.64 MiB/s, done.\n",
      "Resolving deltas: 100% (334/334), done.\n",
      "Updating files: 100% (566/566), done.\n",
      "Processing /workspace/segment-anything-2\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting torch>=2.3.1 (from SAM-2==1.0)\n",
      "  Using cached torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting torchvision>=0.18.1 (from SAM-2==1.0)\n",
      "  Downloading torchvision-0.20.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting numpy>=1.24.4 (from SAM-2==1.0)\n",
      "  Downloading numpy-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m805.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting tqdm>=4.66.1 (from SAM-2==1.0)\n",
      "  Downloading tqdm-4.67.0-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting hydra-core>=1.3.2 (from SAM-2==1.0)\n",
      "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting iopath>=0.1.10 (from SAM-2==1.0)\n",
      "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pillow>=9.4.0 (from SAM-2==1.0)\n",
      "  Downloading pillow-11.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Collecting omegaconf<2.4,>=2.2 (from hydra-core>=1.3.2->SAM-2==1.0)\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.3.2->SAM-2==1.0)\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from hydra-core>=1.3.2->SAM-2==1.0) (23.2)\n",
      "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from iopath>=0.1.10->SAM-2==1.0) (4.4.0)\n",
      "Collecting portalocker (from iopath>=0.1.10->SAM-2==1.0)\n",
      "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.1->SAM-2==1.0) (3.9.0)\n",
      "Collecting typing_extensions (from iopath>=0.1.10->SAM-2==1.0)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.1->SAM-2==1.0) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.1->SAM-2==1.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.1->SAM-2==1.0) (2023.4.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.3.1->SAM-2==1.0)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.3.1->SAM-2==1.0)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.3.1->SAM-2==1.0)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.3.1->SAM-2==1.0)\n",
      "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.3.1->SAM-2==1.0)\n",
      "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.3.1->SAM-2==1.0)\n",
      "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.3.1->SAM-2==1.0)\n",
      "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.3.1->SAM-2==1.0)\n",
      "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.3.1->SAM-2==1.0)\n",
      "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch>=2.3.1->SAM-2==1.0)\n",
      "  Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch>=2.3.1->SAM-2==1.0)\n",
      "  Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.3.1->SAM-2==1.0)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.1.0 (from torch>=2.3.1->SAM-2==1.0)\n",
      "  Using cached triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting sympy==1.13.1 (from torch>=2.3.1->SAM-2==1.0)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.3.1->SAM-2==1.0) (1.3.0)\n",
      "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.4,>=2.2->hydra-core>=1.3.2->SAM-2==1.0) (6.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.3.1->SAM-2==1.0) (2.1.2)\n",
      "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pillow-11.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl (906.4 MB)\n",
      "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
      "Downloading torchvision-0.20.1-cp310-cp310-manylinux1_x86_64.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m130.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.0-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
      "Building wheels for collected packages: SAM-2, antlr4-python3-runtime, iopath\n",
      "  Building wheel for SAM-2 (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for SAM-2: filename=SAM_2-1.0-cp310-cp310-linux_x86_64.whl size=169415 sha256=cd531f3a071eef8296318bce615295a256d41a0f48451c9b8ac74291a67bfea2\n",
      "  Stored in directory: /root/.cache/pip/wheels/d0/2d/19/102588b3e7a3d59ec09aeae7445096bed2703726582d44eb25\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144555 sha256=1bb617c665c7780b536172f737526d16e4495928e7c74da57daf1a13b205c298\n",
      "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
      "  Building wheel for iopath (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31532 sha256=b926557671064a8b590755a915b623e53bbfb531b67fb46373c1acf282d62eb9\n",
      "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
      "Successfully built SAM-2 antlr4-python3-runtime iopath\n",
      "Installing collected packages: antlr4-python3-runtime, typing_extensions, triton, tqdm, sympy, portalocker, pillow, omegaconf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, iopath, hydra-core, nvidia-cusolver-cu12, torch, torchvision, SAM-2\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.1.0\n",
      "    Uninstalling triton-2.1.0:\n",
      "      Successfully uninstalled triton-2.1.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.12\n",
      "    Uninstalling sympy-1.12:\n",
      "      Successfully uninstalled sympy-1.12\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: Pillow 9.3.0\n",
      "    Uninstalling Pillow-9.3.0:\n",
      "      Successfully uninstalled Pillow-9.3.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.1\n",
      "    Uninstalling numpy-1.24.1:\n",
      "      Successfully uninstalled numpy-1.24.1\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.1.0+cu118\n",
      "    Uninstalling torch-2.1.0+cu118:\n",
      "      Successfully uninstalled torch-2.1.0+cu118\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.16.0+cu118\n",
      "    Uninstalling torchvision-0.16.0+cu118:\n",
      "      Successfully uninstalled torchvision-0.16.0+cu118\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed SAM-2-1.0 antlr4-python3-runtime-4.9.3 hydra-core-1.3.2 iopath-0.1.10 numpy-2.1.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 omegaconf-2.3.0 pillow-11.0.0 portalocker-2.10.1 sympy-1.13.1 torch-2.5.1 torchvision-0.20.1 tqdm-4.67.0 triton-3.1.0 typing_extensions-4.12.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/facebookresearch/segment-anything-2.git\n",
    "!cd segment-anything-2; pip3 install ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b9b40d-0313-42e3-9736-d59f8b651115",
   "metadata": {},
   "source": [
    "# Install ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "686dbc34-8a09-41d3-bec0-a3dc1a09c5ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting onnx\n",
      "  Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting onnxscript\n",
      "  Downloading onnxscript-0.1.0.dev20241112-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting onnxsim\n",
      "  Downloading onnxsim-0.4.36-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
      "Collecting onnxruntime\n",
      "  Downloading onnxruntime-1.20.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from onnx) (2.1.3)\n",
      "Collecting protobuf>=3.20.2 (from onnx)\n",
      "  Downloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from onnxscript) (4.12.2)\n",
      "Collecting ml-dtypes (from onnxscript)\n",
      "  Downloading ml_dtypes-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxscript) (23.2)\n",
      "Collecting rich (from onnxsim)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting coloredlogs (from onnxruntime)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime)\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.13.1)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->onnxsim)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->onnxsim) (2.16.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->onnxsim)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m109.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading onnxscript-0.1.0.dev20241112-py3-none-any.whl (720 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m720.8/720.8 kB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading onnxsim-0.4.36-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading onnxruntime-1.20.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl (316 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.6/316.6 kB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading ml_dtypes-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m95.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: flatbuffers, protobuf, ml-dtypes, mdurl, humanfriendly, onnx, markdown-it-py, coloredlogs, rich, onnxscript, onnxruntime, onnxsim\n",
      "Successfully installed coloredlogs-15.0.1 flatbuffers-24.3.25 humanfriendly-10.0 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.5.0 onnx-1.17.0 onnxruntime-1.20.0 onnxscript-0.1.0.dev20241112 onnxsim-0.4.36 protobuf-5.28.3 rich-13.9.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install onnx onnxscript onnxsim onnxruntime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28bce33-5c85-4d0a-b5e6-86cce2024593",
   "metadata": {},
   "source": [
    "# Split model into Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4171d572-07c7-498f-9b62-d5ae3de6a4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple, Any\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.init import trunc_normal_\n",
    "\n",
    "from sam2.modeling.sam2_base import SAM2Base\n",
    "\n",
    "class SAM2ImageEncoder(nn.Module):\n",
    "    def __init__(self, sam_model: SAM2Base) -> None:\n",
    "        super().__init__()\n",
    "        self.model = sam_model\n",
    "        self.image_encoder = sam_model.image_encoder\n",
    "        self.no_mem_embed = sam_model.no_mem_embed\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[Any, Any, Any]:\n",
    "        backbone_out = self.image_encoder(x)\n",
    "        backbone_out[\"backbone_fpn\"][0] = self.model.sam_mask_decoder.conv_s0(\n",
    "            backbone_out[\"backbone_fpn\"][0]\n",
    "        )\n",
    "        backbone_out[\"backbone_fpn\"][1] = self.model.sam_mask_decoder.conv_s1(\n",
    "            backbone_out[\"backbone_fpn\"][1]\n",
    "        )\n",
    "\n",
    "        feature_maps = backbone_out[\"backbone_fpn\"][-self.model.num_feature_levels:]\n",
    "        vision_pos_embeds = backbone_out[\"vision_pos_enc\"][-self.model.num_feature_levels:]\n",
    "\n",
    "        feat_sizes = [(x.shape[-2], x.shape[-1]) for x in vision_pos_embeds]\n",
    "\n",
    "        # flatten NxCxHxW to HWxNxC\n",
    "        vision_feats = [x.flatten(2).permute(2, 0, 1) for x in feature_maps]\n",
    "        vision_pos_embeds = [x.flatten(2).permute(2, 0, 1) for x in vision_pos_embeds]\n",
    "\n",
    "        vision_feats[-1] = vision_feats[-1] + self.no_mem_embed\n",
    "\n",
    "        feats = [feat.permute(1, 2, 0).reshape(1, -1, *feat_size)\n",
    "                 for feat, feat_size in zip(vision_feats[::-1], feat_sizes[::-1])][::-1]\n",
    "\n",
    "        return feats[0], feats[1], feats[2]\n",
    "\n",
    "\n",
    "class SAM2ImageDecoder(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            sam_model: SAM2Base,\n",
    "            multimask_output: bool\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.mask_decoder = sam_model.sam_mask_decoder\n",
    "        self.prompt_encoder = sam_model.sam_prompt_encoder\n",
    "        self.model = sam_model\n",
    "        self.multimask_output = multimask_output\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(\n",
    "            self,\n",
    "            image_embed: torch.Tensor,\n",
    "            high_res_feats_0: torch.Tensor,\n",
    "            high_res_feats_1: torch.Tensor,\n",
    "            point_coords: torch.Tensor,\n",
    "            point_labels: torch.Tensor,\n",
    "            mask_input: torch.Tensor,\n",
    "            has_mask_input: torch.Tensor,\n",
    "            img_size: torch.Tensor\n",
    "    ):\n",
    "        sparse_embedding = self._embed_points(point_coords, point_labels)\n",
    "        self.sparse_embedding = sparse_embedding\n",
    "        dense_embedding = self._embed_masks(mask_input, has_mask_input)\n",
    "\n",
    "        high_res_feats = [high_res_feats_0, high_res_feats_1]\n",
    "        image_embed = image_embed\n",
    "\n",
    "        masks, iou_predictions, _, _ = self.mask_decoder.predict_masks(\n",
    "            image_embeddings=image_embed,\n",
    "            image_pe=self.prompt_encoder.get_dense_pe(),\n",
    "            sparse_prompt_embeddings=sparse_embedding,\n",
    "            dense_prompt_embeddings=dense_embedding,\n",
    "            repeat_image=False,\n",
    "            high_res_features=high_res_feats,\n",
    "        )\n",
    "\n",
    "        if self.multimask_output:\n",
    "            masks = masks[:, 1:, :, :]\n",
    "            iou_predictions = iou_predictions[:, 1:]\n",
    "        else:\n",
    "            masks, iou_predictions = self.mask_decoder._dynamic_multimask_via_stability(masks, iou_predictions)\n",
    "\n",
    "        masks = torch.clamp(masks, -32.0, 32.0)\n",
    "        print(masks.shape, iou_predictions.shape)\n",
    "\n",
    "        masks = F.interpolate(masks, (img_size[0], img_size[1]), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        return masks, iou_predictions\n",
    "\n",
    "    def _embed_points(self, point_coords: torch.Tensor, point_labels: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        point_coords = point_coords + 0.5\n",
    "\n",
    "        padding_point = torch.zeros((point_coords.shape[0], 1, 2), device=point_coords.device)\n",
    "        padding_label = -torch.ones((point_labels.shape[0], 1), device=point_labels.device)\n",
    "        point_coords = torch.cat([point_coords, padding_point], dim=1)\n",
    "        point_labels = torch.cat([point_labels, padding_label], dim=1)\n",
    "\n",
    "        point_coords[:, :, 0] = point_coords[:, :, 0] / self.model.image_size\n",
    "        point_coords[:, :, 1] = point_coords[:, :, 1] / self.model.image_size\n",
    "\n",
    "        point_embedding = self.prompt_encoder.pe_layer._pe_encoding(point_coords)\n",
    "        point_labels = point_labels.unsqueeze(-1).expand_as(point_embedding)\n",
    "\n",
    "        point_embedding = point_embedding * (point_labels != -1)\n",
    "        point_embedding = point_embedding + self.prompt_encoder.not_a_point_embed.weight * (\n",
    "                point_labels == -1\n",
    "        )\n",
    "\n",
    "        for i in range(self.prompt_encoder.num_point_embeddings):\n",
    "            point_embedding = point_embedding + self.prompt_encoder.point_embeddings[i].weight * (point_labels == i)\n",
    "\n",
    "        return point_embedding\n",
    "\n",
    "    def _embed_masks(self, input_mask: torch.Tensor, has_mask_input: torch.Tensor) -> torch.Tensor:\n",
    "        mask_embedding = has_mask_input * self.prompt_encoder.mask_downscaling(input_mask)\n",
    "        mask_embedding = mask_embedding + (\n",
    "                1 - has_mask_input\n",
    "        ) * self.prompt_encoder.no_mask_embed.weight.reshape(1, -1, 1, 1)\n",
    "        return mask_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b4d668-cc9d-4bea-8134-665c14210c8a",
   "metadata": {},
   "source": [
    "# Convert model to .onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf618cb-3e3e-4f96-a1a4-7e4e7d32d36d",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "828045eb-5a3d-40f0-a615-dfc30e95595f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 256, 256])\n",
      "torch.Size([1, 64, 128, 128])\n",
      "torch.Size([1, 256, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sam2/modeling/backbones/utils.py:30: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if pad_h > 0 or pad_w > 0:\n",
      "/usr/local/lib/python3.10/dist-packages/sam2/modeling/backbones/utils.py:60: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if Hp > H or Wp > W:\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sam2.build_sam import build_sam2\n",
    "\n",
    "model_type = \"sam2_hiera_tiny\"\n",
    "model_cfg = \"sam2_hiera_t.yaml\"\n",
    "input_size = 1024 \n",
    "multimask_output = True\n",
    "\n",
    "sam2_checkpoint = f\"./{model_type}.pt\"\n",
    "sam2_model = build_sam2(model_cfg, sam2_checkpoint, device=\"cpu\")\n",
    "\n",
    "img=torch.randn(1, 3, input_size, input_size).cpu()\n",
    "\n",
    "sam2_encoder = SAM2ImageEncoder(sam2_model).cpu()\n",
    "high_res_feats_0, high_res_feats_1, image_embed = sam2_encoder(img)\n",
    "print(high_res_feats_0.shape)\n",
    "print(high_res_feats_1.shape)\n",
    "print(image_embed.shape)\n",
    "\n",
    "torch.onnx.export(sam2_encoder,\n",
    "      img,\n",
    "      f\"{model_type}_encoder.onnx\",\n",
    "      export_params=True,\n",
    "      opset_version=17,\n",
    "      do_constant_folding=True,\n",
    "      input_names = ['image'],\n",
    "      output_names = ['high_res_feats_0', 'high_res_feats_1', 'image_embed'],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26602e82-4e39-4a87-b8dd-d0836b640a86",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1be325f9-ba22-44a4-bb5f-acfe49101392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 (64, 64) [256, 256]\n",
      "torch.Size([1, 3, 256, 256]) torch.Size([1, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sam2/modeling/sam/mask_decoder.py:203: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert image_embeddings.shape[0] == tokens.shape[0]\n",
      "/usr/local/lib/python3.10/dist-packages/sam2/modeling/sam/mask_decoder.py:207: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  image_pe.size(0) == 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256]) torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "sam2_decoder = SAM2ImageDecoder(sam2_model, multimask_output=multimask_output).cpu()\n",
    "\n",
    "embed_dim = sam2_model.sam_prompt_encoder.embed_dim\n",
    "embed_size = (sam2_model.image_size // sam2_model.backbone_stride, sam2_model.image_size // sam2_model.backbone_stride)\n",
    "mask_input_size = [4 * x for x in embed_size]\n",
    "print(embed_dim, embed_size, mask_input_size)\n",
    "\n",
    "point_coords = torch.randint(low=0, high=input_size, size=(1, 5, 2), dtype=torch.float)\n",
    "point_labels = torch.randint(low=0, high=1, size=(1, 5), dtype=torch.float)\n",
    "mask_input = torch.randn(1, 1, *mask_input_size, dtype=torch.float)\n",
    "has_mask_input = torch.tensor([1], dtype=torch.float)\n",
    "orig_im_size = torch.tensor([input_size, input_size], dtype=torch.int32)\n",
    "\n",
    "masks, scores = sam2_decoder(image_embed, high_res_feats_0, high_res_feats_1, point_coords, point_labels, mask_input, has_mask_input, orig_im_size)\n",
    "\n",
    "torch.onnx.export(sam2_decoder,\n",
    "      (image_embed, high_res_feats_0, high_res_feats_1, point_coords, point_labels, mask_input, has_mask_input, orig_im_size),\n",
    "      f\"{model_type}_decoder.onnx\",\n",
    "      export_params=True,\n",
    "      opset_version=16,\n",
    "      do_constant_folding=True,\n",
    "      input_names = ['image_embed', 'high_res_feats_0', 'high_res_feats_1', 'point_coords', 'point_labels', 'mask_input', 'has_mask_input', 'orig_im_size'],\n",
    "      output_names = ['masks', 'iou_predictions'],\n",
    "      dynamic_axes = {\"point_coords\": {0: \"num_labels\", 1: \"num_points\"},\n",
    "                      \"point_labels\": {0: \"num_labels\", 1: \"num_points\"},\n",
    "                      \"mask_input\": {0: \"num_labels\"},\n",
    "                      \"has_mask_input\": {0: \"num_labels\"}\n",
    "      }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e41306-d443-4060-90ff-8beb39f37ba4",
   "metadata": {},
   "source": [
    "# Test exported models with `onnxruntime`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99a43dc-1077-46fc-8d64-734f88ffe366",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88e93dad-28e0-4caf-91a6-0b114791307f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "onnx_model = onnx.load(\"sam2_hiera_tiny_encoder.onnx\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7280b92-6010-48fd-9ba6-0aad26cba989",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2024-11-15 09:48:35.666534734 [W:onnxruntime:, graph.cc:109 MergeShapeInfo] Error merging shape info for output. '/image_encoder/trunk/Concat_3_output_0' source:{4} target:{5}. Falling back to lenient merge.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "ort_sess = ort.InferenceSession('sam2_hiera_tiny_encoder.onnx', {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "451abb08-fb64-4a8c-bc05-cf6a51746a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "high_res_feats_0\n",
      "(1, 32, 256, 256)\n",
      "high_res_feats_1\n",
      "(1, 64, 128, 128)\n",
      "image_embed\n",
      "(1, 256, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "img = torch.randn(1, 3, input_size, input_size).cpu()\n",
    "\n",
    "outputs = ort_sess.run(None, {\n",
    "    'image': img.numpy(),\n",
    "})\n",
    "\n",
    "for i in range(len(outputs)):\n",
    "    print (ort_sess.get_outputs()[i].name)\n",
    "    print (outputs[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d213286a-9633-4c8c-9743-5bab76ba386d",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5237ada8-9e49-49a3-9673-14eb16bf41bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = onnx.load(\"sam2_hiera_tiny_decoder.onnx\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecf5fefd-1bb5-4629-b212-4f96863860f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masks\n",
      "(1, 3, 1024, 1024)\n",
      "iou_predictions\n",
      "(1, 3)\n"
     ]
    }
   ],
   "source": [
    "ort_sess = ort.InferenceSession('sam2_hiera_tiny_decoder.onnx', {})\n",
    "\n",
    "outputs = ort_sess.run(None, {\n",
    "    'image_embed': image_embed.detach().numpy(), \n",
    "    'high_res_feats_0': high_res_feats_0.detach().numpy(), \n",
    "    'high_res_feats_1': high_res_feats_1.detach().numpy(), \n",
    "    'point_coords': point_coords.numpy(), \n",
    "    'point_labels': point_labels.numpy(), \n",
    "    'mask_input': mask_input.numpy(), \n",
    "    'has_mask_input': has_mask_input.numpy(), \n",
    "    'orig_im_size': orig_im_size.numpy()\n",
    "})\n",
    "\n",
    "for i in range(len(outputs)):\n",
    "    print (ort_sess.get_outputs()[i].name)\n",
    "    print (outputs[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7929ff-28c0-43cb-bdec-1f614b7f85dd",
   "metadata": {},
   "source": [
    "# Convert Encoder model .onnx to .ort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7268cbb-cef3-4ad2-bb5c-0f5b1d004847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting models with optimization style 'Fixed' and level 'all'\n",
      "Converting optimized ONNX model /workspace/sam2_hiera_tiny_encoder.onnx to ORT format model /workspace/sam2_hiera_tiny_encoder.ort\n",
      "\u001b[0;93m2024-11-15 09:49:00.758685864 [W:onnxruntime:, graph.cc:109 MergeShapeInfo] Error merging shape info for output. '/image_encoder/trunk/Concat_3_output_0' source:{4} target:{5}. Falling back to lenient merge.\u001b[m\n",
      "Converted 1/1 models successfully.\n",
      "Generating config file from ORT format models with optimization style 'Fixed' and level 'all'\n",
      "2024-11-15 09:49:03,669 ort_format_model.utils [INFO] - Created config in /workspace/sam2_hiera_tiny_encoder.required_operators.config\n",
      "Converting models with optimization style 'Runtime' and level 'all'\n",
      "Converting optimized ONNX model /workspace/sam2_hiera_tiny_encoder.onnx to ORT format model /workspace/sam2_hiera_tiny_encoder.with_runtime_opt.ort\n",
      "\u001b[0;93m2024-11-15 09:49:04.882172447 [W:onnxruntime:, graph.cc:109 MergeShapeInfo] Error merging shape info for output. '/image_encoder/trunk/Concat_3_output_0' source:{4} target:{5}. Falling back to lenient merge.\u001b[m\n",
      "Converted 1/1 models successfully.\n",
      "Converting models again without runtime optimizations to generate a complete config file. These converted models are temporary and will be deleted.\n",
      "Converting optimized ONNX model /workspace/sam2_hiera_tiny_encoder.onnx to ORT format model /workspace/tmpyjg0qnj5.without_runtime_opt/sam2_hiera_tiny_encoder.ort\n",
      "\u001b[0;93m2024-11-15 09:49:08.319962455 [W:onnxruntime:, graph.cc:109 MergeShapeInfo] Error merging shape info for output. '/image_encoder/trunk/Concat_3_output_0' source:{4} target:{5}. Falling back to lenient merge.\u001b[m\n",
      "Converted 1/1 models successfully.\n",
      "Generating config file from ORT format models with optimization style 'Runtime' and level 'all'\n",
      "2024-11-15 09:49:10,893 ort_format_model.utils [INFO] - Created config in /workspace/sam2_hiera_tiny_encoder.required_operators.with_runtime_opt.config\n"
     ]
    }
   ],
   "source": [
    "!python3 -m onnxruntime.tools.convert_onnx_models_to_ort sam2_hiera_tiny_encoder.onnx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
